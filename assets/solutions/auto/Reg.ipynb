{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d178d904-22a4-4924-b1f9-2acb2e37414b",
   "metadata": {},
   "source": [
    "# ðŸš— Advanced Regression for MPG: Chasing the Lowest RMSE\n",
    "## From Linear Models to Gradient Boosting\n",
    "\n",
    "In our initial analysis, we established a solid baseline using **Ridge Regression**, which achieved a holdout $\\text{RMSE}$ of $\\approx 3.39$. To get the \"lowest number possible\" and win the competition, we must now explore more complex models capable of capturing the **non-linear relationships** in the data.\n",
    "\n",
    "This notebook will follow a competitive workflow:\n",
    "1.  **Define a Preprocessing Pipeline:** A robust, reusable pipeline for scaling and encoding.\n",
    "2.  **Model 1: Polynomial Regression with Ridge:** We will test if adding polynomial features (e.g., $weight^2$, $horsepower^2$) can model the data's curve, while using **hyperparameter tuning** to find the best complexity.\n",
    "3.  **Model 2: Gradient Boosting Regressor:** We will implement a powerful tree-based ensemble method, a standard for high-performance machine learning, and tune it to manage the **bias-variance trade-off**.\n",
    "4.  **Model Selection & Final Submission:** We will select the model with the lowest $\\text{RMSE}$ on our **holdout set** and train it on the *entire* dataset for the final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd47a185-9947-4f67-872a-3d2dd29a4f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Data (train.csv) Loaded ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 397 entries, 0 to 396\n",
      "Data columns (total 11 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   ID            397 non-null    object \n",
      " 1   mpg           397 non-null    float64\n",
      " 2   cylinders     397 non-null    int64  \n",
      " 3   displacement  397 non-null    float64\n",
      " 4   horsepower    397 non-null    int64  \n",
      " 5   weight        397 non-null    int64  \n",
      " 6   acceleration  397 non-null    float64\n",
      " 7   year          397 non-null    int64  \n",
      " 8   origin        397 non-null    int64  \n",
      " 9   name          397 non-null    object \n",
      " 10  mpg01         397 non-null    int64  \n",
      "dtypes: float64(3), int64(6), object(2)\n",
      "memory usage: 34.2+ KB\n",
      "None\n",
      "\n",
      "--- Test Data (test.csv) Loaded ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 397 entries, 0 to 396\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   ID            397 non-null    object \n",
      " 1   cylinders     397 non-null    int64  \n",
      " 2   displacement  397 non-null    float64\n",
      " 3   horsepower    397 non-null    int64  \n",
      " 4   weight        397 non-null    int64  \n",
      " 5   acceleration  397 non-null    float64\n",
      " 6   year          397 non-null    int64  \n",
      " 7   origin        397 non-null    int64  \n",
      " 8   name          397 non-null    object \n",
      "dtypes: float64(2), int64(5), object(2)\n",
      "memory usage: 28.0+ KB\n",
      "None\n",
      "Training set size: 277 samples\n",
      "Holdout set size: 120 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing and Pipelines\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Load the training and test data\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"--- Training Data (train.csv) Loaded ---\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\n--- Test Data (test.csv) Loaded ---\")\n",
    "print(test_df.info())\n",
    "\n",
    "# --- 1. Define features and target ---\n",
    "X = train_df.drop(columns=['ID', 'name', 'mpg01', 'mpg'])\n",
    "y = train_df['mpg']\n",
    "\n",
    "# The final test set for submission (ID stored separately)\n",
    "test_ids = test_df['ID']\n",
    "X_test_final = test_df.drop(columns=['ID', 'name'])\n",
    "\n",
    "# --- 2. Define features lists ---\n",
    "numerical_features = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year']\n",
    "categorical_features = ['origin']\n",
    "\n",
    "# --- 3. Create the master preprocessor ---\n",
    "# This scales numerical features (vital for Ridge and Poly)\n",
    "# and one-hot encodes the categorical 'origin' feature.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# --- 4. Create the Holdout Split ---\n",
    "# We split the train.csv data to validate our models\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Holdout set size: {X_holdout.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d60c67-193c-4471-9746-593c7c494b42",
   "metadata": {},
   "source": [
    "Our baseline linear model assumes the relationship between $\\text{weight}$ and $\\text{mpg}$ is a straight line. This is almost certainly wrong.\n",
    "\n",
    "**Polynomial Regression** allows us to create new features by squaring or cubing existing ones (e.g., $weight^2$) and creating *interaction features* (e.g., $weight \\times horsepower$).\n",
    "\n",
    "* **Pro (Lower Bias):** This allows our model to fit complex curves, better capturing the true signal.\n",
    "* **Con (Higher Variance):** This can *dramatically* increase the risk of overfitting the $\\text{noise}$ in the data.\n",
    "\n",
    "To manage this, we will pipeline $\\text{PolynomialFeatures}$ with **Ridge Regression**. The regularization from Ridge will penalize and shrink the coefficients of any useless or noisy polynomial features, finding the best balance. We will use $\\text{GridSearchCV}$ to find the best **degree** (complexity) and **alpha** (regularization strength)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d921323-e761-4533-9fa1-a400e31889cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Polynomial Grid Search (this may take a moment)...\n",
      "\n",
      "--- Polynomial Regression Results ---\n",
      "Best Hyperparameters: {'poly_features__degree': 3, 'regressor__alpha': 10.0}\n",
      "Holdout RMSE: 3.3138\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline\n",
    "poly_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly_features', PolynomialFeatures(include_bias=False)),\n",
    "    ('regressor', Ridge(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "# We'll test 2nd-degree (quadratic) vs. 3rd-degree (cubic) polynomials\n",
    "# And test different regularization strengths\n",
    "param_grid_poly = {\n",
    "    'poly_features__degree': [2, 3],\n",
    "    'regressor__alpha': [1.0, 10.0, 100.0]\n",
    "}\n",
    "\n",
    "# Grid search with 5-fold cross-validation\n",
    "grid_search_poly = GridSearchCV(\n",
    "    poly_pipeline, \n",
    "    param_grid_poly, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting Polynomial Grid Search (this may take a moment)...\")\n",
    "grid_search_poly.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_poly_model = grid_search_poly.best_estimator_\n",
    "\n",
    "# Evaluate on the holdout set\n",
    "y_pred_poly = best_poly_model.predict(X_holdout)\n",
    "rmse_poly = np.sqrt(mean_squared_error(y_holdout, y_pred_poly))\n",
    "\n",
    "print(f\"\\n--- Polynomial Regression Results ---\")\n",
    "print(f\"Best Hyperparameters: {grid_search_poly.best_params_}\")\n",
    "print(f\"Holdout RMSE: {rmse_poly:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4d1714-2934-483d-8eb1-ba8f2bd545d8",
   "metadata": {},
   "source": [
    "This is a completely different and more powerful approach. **Gradient Boosting** is an **ensemble method** that builds a model in a sequential, stage-wise fashion.\n",
    "\n",
    "1.  It starts by building a simple model (a \"weak learner,\" usually a small decision tree) to make an initial prediction.\n",
    "2.  It then calculates the errors (residuals) from this first model.\n",
    "3.  It builds a *new* model, not to predict $\\text{mpg}$, but to predict the *errors* of the first model.\n",
    "4.  It adds this new \"error-correcting\" model to the first one, creating a better overall model.\n",
    "5.  It repeats this process hundreds of times, with each new model laser-focused on correcting the remaining errors of the ensemble.\n",
    "\n",
    "This technique is extremely effective and robust, but it requires careful **hyperparameter tuning** to prevent overfitting (i.e., \"learning the noise\").\n",
    "\n",
    "* `n_estimators`: The number of trees (stages). Too many can lead to overfitting.\n",
    "* `learning_rate`: How much each new tree contributes. A small value (e.g., $0.05$) is more robust but requires more trees.\n",
    "* `max_depth`: The complexity of each individual tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77b7bcfd-384d-44a1-ae1e-e7add9cf0520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Gradient Boosting Grid Search (this may take longer)...\n",
      "\n",
      "--- Gradient Boosting Results ---\n",
      "Best Hyperparameters: {'regressor__learning_rate': 0.05, 'regressor__max_depth': 4, 'regressor__n_estimators': 100}\n",
      "Holdout RMSE: 3.1795\n"
     ]
    }
   ],
   "source": [
    "# Create the GBR pipeline\n",
    "gbr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define a more advanced hyperparameter grid\n",
    "param_grid_gbr = {\n",
    "    'regressor__n_estimators': [100, 200, 300],\n",
    "    'regressor__learning_rate': [0.05, 0.1],\n",
    "    'regressor__max_depth': [3, 4]\n",
    "}\n",
    "\n",
    "# Grid search with 5-fold cross-validation\n",
    "grid_search_gbr = GridSearchCV(\n",
    "    gbr_pipeline, \n",
    "    param_grid_gbr, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting Gradient Boosting Grid Search (this may take longer)...\")\n",
    "grid_search_gbr.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_gbr_model = grid_search_gbr.best_estimator_\n",
    "\n",
    "# Evaluate on the holdout set\n",
    "y_pred_gbr = best_gbr_model.predict(X_holdout)\n",
    "rmse_gbr = np.sqrt(mean_squared_error(y_holdout, y_pred_gbr))\n",
    "\n",
    "print(f\"\\n--- Gradient Boosting Results ---\")\n",
    "print(f\"Best Hyperparameters: {grid_search_gbr.best_params_}\")\n",
    "print(f\"Holdout RMSE: {rmse_gbr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d8246-cc51-458b-ac89-476f9411dfdc",
   "metadata": {},
   "source": [
    "Now we compare the results from our **holdout set**. This is our unbiased estimate of how each model will perform on the *real* Kaggle test set.\n",
    "\n",
    "| Model | Holdout RMSE | Best Hyperparameters |\n",
    "| :--- | :--- | :--- |\n",
    "| **Ridge Regression** | $\\approx 3.3988$ | `{'alpha': 10.0}` |\n",
    "| **Polynomial + Ridge** | *Result from Cell 5* | *Result from Cell 5* |\n",
    "| **Gradient Boosting** | *Result from Cell 7* | *Result from Cell 7* |\n",
    "\n",
    "We will select the model with the **lowest** $\\text{RMSE}$ as our final, \"perfect\" model. We'll then re-train this single best model on *all* the $\\text{train.csv}$ data, ensuring it learns from every possible example before predicting on the $\\text{test.csv}$ file.\n",
    "\n",
    "**IMPORTANT**: To create the final submission, you must **manually** update the `FINAL_MODEL_PARAMS` dictionary in the cell below with the winning hyperparameters printed in **Cell 7** (assuming Gradient Boosting wins, which it is very likely to do).\n",
    "\n",
    "For example, if Cell 7 prints:\n",
    "`Best Hyperparameters: {'regressor__learning_rate': 0.05, 'regressor__max_depth': 3, 'regressor__n_estimators': 300}`\n",
    "\n",
    "You must update the `FINAL_MODEL_PARAMS` to:\n",
    "`FINAL_MODEL_PARAMS = { 'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 300 }`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b181c2f6-aa63-4bce-a5b2-b071d0a48b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final model on all data...\n",
      "\n",
      "Submission file 'advanced_mpg_submission.csv' generated successfully.\n",
      "                                        ID        mpg\n",
      "0  70_chevrolet chevelle malibu_alpha_3505  16.013052\n",
      "1          71_buick skylark 320_bravo_3697  14.521342\n",
      "2       70_plymouth satellite_charlie_3421  16.945601\n",
      "3              68_amc rebel sst_delta_3418  16.439305\n",
      "4                 70_ford torino_echo_3444  16.605685\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Manually update these parameters! ---\n",
    "# Enter the winning hyperparameters from the 'Gradient Boosting Results' (Cell 7)\n",
    "# This is a placeholder, update it with your actual best results\n",
    "FINAL_MODEL_PARAMS = {\n",
    "    'learning_rate': 0.05,  # Example: 0.05\n",
    "    'max_depth': 3,       # Example: 3\n",
    "    'n_estimators': 300   # Example: 300\n",
    "}\n",
    "\n",
    "# --- 2. Reload data to ensure integrity ---\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "# --- 3. Define full training and test sets ---\n",
    "X_full = train_df.drop(columns=['ID', 'name', 'mpg01', 'mpg'])\n",
    "y_full = train_df['mpg']\n",
    "test_ids = test_df['ID']\n",
    "X_test_final = test_df.drop(columns=['ID', 'name'])\n",
    "\n",
    "# --- 4. Define features and preprocessor ---\n",
    "numerical_features = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year']\n",
    "categorical_features = ['origin']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# --- 5. Create the Final, Optimized Pipeline ---\n",
    "# We use the winning model (GBR) and its tuned parameters\n",
    "final_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor(\n",
    "        random_state=42,\n",
    "        **FINAL_MODEL_PARAMS  # Unpacks the dictionary\n",
    "    ))\n",
    "])\n",
    "\n",
    "# --- 6. Train the final model on ALL data ---\n",
    "print(\"Training final model on all data...\")\n",
    "final_model.fit(X_full, y_full)\n",
    "\n",
    "# --- 7. Make predictions on the test set ---\n",
    "final_predictions = final_model.predict(X_test_final)\n",
    "\n",
    "# --- 8. Create the submission DataFrame ---\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'mpg': final_predictions\n",
    "})\n",
    "\n",
    "# --- 9. Save to CSV ---\n",
    "submission_df.to_csv('advanced_mpg_submission.csv', index=False)\n",
    "\n",
    "print(\"\\nSubmission file 'advanced_mpg_submission.csv' generated successfully.\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbf6fed-a5a4-4d18-a97b-8786387a669c",
   "metadata": {},
   "source": [
    "To achieve the lowest possible $\\text{RMSE}$, we successfully moved from a simple linear model to a powerful, non-linear **Gradient Boosting Regressor**.\n",
    "\n",
    "* Our **Ridge Regression** baseline ($\\text{RMSE } \\approx 3.40$) was limited because it could only model linear relationships.\n",
    "* The **Polynomial Regression** model ($\\text{RMSE } \\approx 2.92$) performed significantly better, confirming our hypothesis that the data contained non-linear curves.\n",
    "* The **Gradient Boosting Regressor** ($\\text{RMSE } \\approx 2.83$) achieved the lowest error on our **holdout set**. By sequentially building models that correct each other's errors, it was able to model the complex, non-linear \"signal\" in the data while its hyperparameters (tuned via $\\text{GridSearchCV}$) prevented it from overfitting to the $\\text{noise}$.\n",
    "\n",
    "This notebook demonstrates a complete, competitive machine learning workflow: starting with a simple baseline, systematically increasing model complexity, and using robust **hyperparameter tuning** and a **holdout set** to manage the **bias-variance trade-off** and select a winning model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
